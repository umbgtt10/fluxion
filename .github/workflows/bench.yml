name: Benchmarks

on:
  workflow_dispatch:      # Manual "Run workflow" button
  push:
    branches: [main]
  pull_request:
  schedule:
    - cron: '0 4 * * *'    # Daily at 4 AM UTC

jobs:
  bench:
    runs-on: ubuntu-latest
    permissions:
      contents: write       # To commit baseline data
      pages: write         # To deploy to GitHub Pages
      id-token: write      # To verify deployment

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install stable Rust
        uses: dtolnay/rust-toolchain@stable

      - name: Cache cargo registry + target
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-cargo-${{ github.sha }}
          restore-keys: |
            ${{ runner.os }}-cargo-

      - name: Run all benchmarks (all packages)
        run: cargo bench -p fluxion-stream -p fluxion-stream-time -p fluxion-ordered-merge

      # --------------------------------------------------------------
      # 1. ALWAYS create/update baseline from this run (before comparison)
      # --------------------------------------------------------------
      - name: Create/update baseline JSON
        run: |
          mkdir -p benches/baseline

          # Criterion outputs to target/criterion/<bench_name>/ (e.g., combine_latest/, ordered_merge/)
          # Copy all benchmark directories except 'report'
          if [ -d "target/criterion" ]; then
            for dir in target/criterion/*/; do
              dirname=$(basename "$dir")
              if [ "$dirname" != "report" ]; then
                cp -r "$dir" benches/baseline/
              fi
            done
            echo "Baseline updated from this run"
            git add -A benches/baseline
          else
            echo "No benchmark data generated - skipping baseline update"
          fi

      # --------------------------------------------------------------
      # 2. Commit baseline (only on main + nightly)
      # --------------------------------------------------------------
      - name: Commit baseline to repo (on main/schedule)
        if: github.ref == 'refs/heads/main' || github.event_name == 'schedule'
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "chore: update benchmark baseline [ci skip]"
          file_pattern: 'benches/baseline/**'
          commit_user_name: github-actions[bot]
          commit_user_email: github-actions[bot]@users.noreply.github.com
          skip_dirty_check: false
          skip_fetch: false
          skip_checkout: false

      # --------------------------------------------------------------
      # 3. Compare + fail on regressions
      # --------------------------------------------------------------
      - name: Install critcmp
        run: cargo install critcmp --locked

      - name: Regression check (>10% slowdown = fail)
        run: |
          set -euo pipefail

          # Skip if baseline doesn't exist yet (first run)
          # Check for any benchmark directories in baseline (excluding report)
          baseline_count=$(find benches/baseline -mindepth 1 -maxdepth 1 -type d 2>/dev/null | wc -l)
          criterion_count=$(find target/criterion -mindepth 1 -maxdepth 1 -type d -not -name report 2>/dev/null | wc -l)

          if [ "$baseline_count" -eq 0 ] || [ "$criterion_count" -eq 0 ]; then
            echo "Skipping regression check â€“ baseline not ready"
            echo "First run - baseline will be established" > benches/comparison.txt
            exit 0
          fi

          # Check if there are any actual benchmark results to compare
          if [ ! "$(find benches/baseline -name '*.json' 2>/dev/null)" ]; then
            echo "No baseline data files found yet"
            echo "Baseline is being established" > benches/comparison.txt
            exit 0
          fi

          echo "Running comparison..."
          critcmp benches/baseline target/criterion --threshold 10 > benches/comparison.txt 2>&1 || {
            # Handle "no comparisons" gracefully
            if grep -qi "no benchmark comparisons\|no benchmarks found" benches/comparison.txt; then
              echo "No changes detected (identical to baseline) â€“ all good!"
              echo "No regressions detected" > benches/comparison.txt
              exit 0
            else
              echo "Comparison failed or found regressions:"
              cat benches/comparison.txt
              # Check if it's a regression or just an error
              if grep -qi "SLOWER\|regression" benches/comparison.txt; then
                exit 1
              else
                # Unknown error, but don't fail the build
                echo "Warning: critcmp comparison had issues but no clear regression detected"
                exit 0
              fi
            fi
          }

          cat benches/comparison.txt

          if grep -qi "SLOWER" benches/comparison.txt; then
            echo "ðŸš¨ Performance regression detected!"
            exit 1
          else
            echo "âœ… All benchmarks stable or improved"
          fi

      - name: Debug - List criterion report contents
        run: |
          echo "=== Contents of target/criterion/report ==="
          ls -laR target/criterion/report/
          echo "=== End of listing ==="

      # --------------------------------------------------------------
      # 4. Deploy to GitHub Pages (on main only) â€” FULL ROOT DEPLOY
      # --------------------------------------------------------------
      - name: Prepare benchmark report for deployment
        if: github.ref == 'refs/heads/main' || github.event_name == 'schedule'
        run: |
          mkdir -p deploy
          # Copy all criterion output directly (not into a subfolder)
          # This way the relative links in report/index.html will work correctly
          cp -r target/criterion/* deploy/

          # The report/index.html has links like "../benchmark_name/..." which work
          # from the report/ subfolder. We want to use it as the main index.html
          # at the benchmarks root, so we need to fix the relative paths.
          # Change "../" to "./" in the copied index.html
          if [ -f deploy/report/index.html ]; then
            sed 's|href="\.\./|href="./|g' deploy/report/index.html > deploy/index.html
            echo "Created deploy/index.html with fixed relative paths"
          fi

          echo "Contents of deploy:"
          ls -la deploy/
          echo "Sample links in index.html:"
          head -100 deploy/index.html | grep -o 'href="[^"]*"' | head -10 || true

      - name: Deploy benchmarks to GitHub Pages
        if: github.ref == 'refs/heads/main' || github.event_name == 'schedule'
        uses: peaceiris/actions-gh-pages@v4
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./deploy
          destination_dir: benchmarks
          keep_files: true  # Preserve other content on gh-pages
          user_name: 'github-actions[bot]'
          user_email: 'github-actions[bot]@users.noreply.github.com'
          commit_message: 'Deploy Fluxion benchmarks to gh-pages'

      # --------------------------------------------------------------
      # 5. Upload report artifact (for PRs and debugging)
      # --------------------------------------------------------------
      - name: Upload Criterion report
        uses: actions/upload-artifact@v4
        if: always()  # Upload even if regression check fails
        with:
          name: benchmark-report
          path: target/criterion/
          retention-days: 14

      # --------------------------------------------------------------
      # 6. PR comment
      # --------------------------------------------------------------
      - name: Comment on PR
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            let comparison = 'Comparison skipped (first run or no changes detected).';
            if (fs.existsSync('benches/comparison.txt')) {
              comparison = fs.readFileSync('benches/comparison.txt', 'utf8').trim();
            }

            const runUrl = process.env.GITHUB_SERVER_URL + '/' + process.env.GITHUB_REPOSITORY + '/actions/runs/' + process.env.GITHUB_RUN_ID + '/artifacts';
            const body = '### ðŸ“Š Benchmark Results (all packages)\n\n' +
                         '```\n' + comparison + '\n```\n\n' +
                         'ðŸ“ˆ Full interactive report (HTML tables + graphs): [Download benchmark-report artifact](' + runUrl + ')';

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body.trim()
            });
